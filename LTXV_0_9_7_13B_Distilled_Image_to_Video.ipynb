{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMAGE TO VIDEO WITH LTXV 0.9.7 13B DISTILLED**"
      ],
      "metadata": {
        "id": "f4p1ysFKMbs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You can use the free T4 GPU to generate a 5-second video (120 frames) in about 10 minutes without upscaling.\n",
        "\n",
        "- For upscaled videos, the T4 GPU can handle up to 1 second (25 frames).\n",
        "\n",
        "- For longer or faster video generation with upscale, consider using higher-tier GPUs.\n",
        "\n",
        "- All videos are generated at 24 FPS.\n",
        "\n",
        "- To generate a video with n frames, set the frames value to n + 1. To create a 5-second video (120 frames), set frames = 121.\n",
        "\n",
        "- The output video will match the resolution of the uploaded image.\n",
        "\n",
        "- Enabling the upscale_video option will generate a second, higher-quality version of the video at twice the original resolution."
      ],
      "metadata": {
        "id": "EBB00lC6q-DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prepare Environment\n",
        "# !pip install --upgrade --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "!pip install --quiet torch torchvision\n",
        "\n",
        "%cd /content\n",
        "Use_t5xxl_fp16 = False\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate\n",
        "# !pip install xformers\n",
        "!pip install av\n",
        "!git clone --branch ComfyUI_v0.3.34 https://github.com/Isi-dev/ComfyUI\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone --branch forHidream https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_LTXVideo\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_LTXVideo\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI\n",
        "!apt -y install -qq aria2 ffmpeg\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import os\n",
        "import imageio\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAELoader,\n",
        "    VAEDecode,\n",
        "    VAEDecodeTiled,\n",
        "    LoadImage,\n",
        "    ImageScale,\n",
        "    SaveImage\n",
        ")\n",
        "\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import (\n",
        "    UnetLoaderGGUF\n",
        ")\n",
        "\n",
        "from comfy_extras.nodes_custom_sampler import (\n",
        "    KSamplerSelect,\n",
        "    SamplerCustom,\n",
        "    RandomNoise\n",
        ")\n",
        "\n",
        "from comfy_extras.nodes_lt import (\n",
        "    LTXVPreprocess,\n",
        "    LTXVImgToVideo,\n",
        "    LTXVScheduler,\n",
        "    LTXVConditioning\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.stg import STGGuiderAdvancedNode\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.easy_samplers import LTXVBaseSampler\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.latent_upsampler import (\n",
        "    LTXVLatentUpsamplerModelLoader,\n",
        "    LTXVLatentUpsampler\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.latent_adain import LTXVAdainLatent\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.tiled_sampler import LTXVTiledSampler\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.film_grain import LTXVFilmGrain\n",
        "\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/wsbagnsv1/ltxv-13b-0.9.7-distilled-GGUF/resolve/main/ltxv-13b-0.9.7-distilled-Q6_K.gguf -d /content/ComfyUI/models/diffusion_models -o ltxv-13b-0.9.7-distilled-Q6_K.gguf\n",
        "if Use_t5xxl_fp16:\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp16.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp16.safetensors\n",
        "else:\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp8_e4m3fn_scaled.safetensors\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/wsbagnsv1/ltxv-13b-0.9.7-dev-GGUF/resolve/main/ltxv-13b-0.9.7-vae-BF16.safetensors -d /content/ComfyUI/models/vae -o ltxv-13b-0.9.7-vae-BF16.safetensors\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-spatial-upscaler-0.9.7.safetensors -d /content/ComfyUI/models/upscale_models -o ltxv-spatial-upscaler-0.9.7.safetensors\n",
        "\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-temporal-upscaler-0.9.7.safetensors -d /content/ComfyUI/models/upscale_models -o ltxv-temporal-upscaler-0.9.7.safetensors\n",
        "\n",
        "clear_output()\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def upload_image():\n",
        "    \"\"\"Handle image upload in Colab and store in /content/ComfyUI/input/\"\"\"\n",
        "    from google.colab import files\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Move each uploaded file to ComfyUI input directory\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "\n",
        "        shutil.move(src_path, dest_path)\n",
        "        print(f\"Image saved to: {dest_path}\")\n",
        "        return dest_path\n",
        "\n",
        "    return None\n",
        "\n",
        "def string_to_float(string):\n",
        "        float_list = [float(x.strip()) for x in string.split(',')]\n",
        "        return (float_list,)\n",
        "\n",
        "def float_to_sigmas(float_list):\n",
        "        return torch.tensor(float_list, dtype=torch.float32),\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def generate_video(\n",
        "    image_path: str = None,\n",
        "    positive_prompt: str = \"A red fox moving gracefully\",\n",
        "    negative_prompt: str = \"low quality, worst quality\",\n",
        "    width: int = 768,\n",
        "    height: int = 512,\n",
        "    seed: int = 0,\n",
        "    steps: int = 30,\n",
        "    cfg_scale: float = 2.05,\n",
        "    sampler_name: str = \"euler\",\n",
        "    length: int = 24,  # Number of frames\n",
        "    fps: int = 24,\n",
        "    upscale_video: bool = False\n",
        "):\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        unet_loader = UnetLoaderGGUF()\n",
        "        vae_loader = VAELoader()\n",
        "        checkpoint_loader = CheckpointLoaderSimple()\n",
        "        clip_loader = CLIPLoader()\n",
        "        clip_encode_positive = CLIPTextEncode()\n",
        "        clip_encode_negative = CLIPTextEncode()\n",
        "        load_image = LoadImage()\n",
        "        image_scaler = ImageScale()\n",
        "        save_node = SaveImage()\n",
        "        preprocess = LTXVPreprocess()\n",
        "        img_to_video = LTXVImgToVideo()\n",
        "        scheduler = LTXVScheduler()\n",
        "        sampler_select = KSamplerSelect()\n",
        "        random_noise = RandomNoise()\n",
        "        conditioning = LTXVConditioning()\n",
        "        sampler = SamplerCustom()\n",
        "        vae_decode = VAEDecode()\n",
        "        stg_guider_advanced = STGGuiderAdvancedNode()\n",
        "        ltxv_base_sampler = LTXVBaseSampler()\n",
        "        vae_decode_tiled = VAEDecodeTiled()\n",
        "        upscale_model_loader = LTXVLatentUpsamplerModelLoader()\n",
        "        latent_upsampler = LTXVLatentUpsampler()\n",
        "        adain_latent = LTXVAdainLatent()\n",
        "        tiled_sampler = LTXVTiledSampler()\n",
        "        film_grain = LTXVFilmGrain()\n",
        "\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(\"t5xxl_fp8_e4m3fn_scaled.safetensors\", \"ltxv\", \"default\")[0]\n",
        "\n",
        "        assert width % 32 == 0, \"Width must be divisible by 32\"\n",
        "        assert height % 32 == 0, \"Height must be divisible by 32\"\n",
        "\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        if image_path is None:\n",
        "            print(\"Please upload an image file:\")\n",
        "            image_path = upload_image()\n",
        "        if image_path is None:\n",
        "            print(\"No image uploaded!\")\n",
        "        loaded_image = load_image.load_image(image_path)[0]\n",
        "        # processed_image = preprocess.preprocess(loaded_image, 40)[0]\n",
        "\n",
        "        width_int, height_int = image_width_height(loaded_image)\n",
        "\n",
        "        if width == 0 and height == 0 :\n",
        "            if width_int > height_int:\n",
        "                width = 768\n",
        "                height = 512\n",
        "            elif width_int == height_int:\n",
        "                width = 512\n",
        "                height = 512\n",
        "            else:\n",
        "                width = 512\n",
        "                height = 768\n",
        "\n",
        "\n",
        "        print(\"Loading UNet model...\")\n",
        "        model = unet_loader.load_unet(\"ltxv-13b-0.9.7-distilled-Q6_K.gguf\")[0]\n",
        "\n",
        "        conditionedPositive, conditionedNegative = conditioning.append(positive, negative, 25.0)\n",
        "\n",
        "        guider = stg_guider_advanced.get_guider(\n",
        "            model,\n",
        "            conditionedPositive,\n",
        "            conditionedNegative,\n",
        "            0.997,  # skip_steps_sigma_threshold\n",
        "            True,    # cfg_star_rescale\n",
        "            \"1.0, 0.9933, 0.9850, 0.9767, 0.9008, 0.6180\",  # sigmas\n",
        "            \"1,1,1,1,1,1\",  # cfg_values\n",
        "            \"0,0,0,0,0,0\",  # stg_scale_values\n",
        "            \"1, 1, 1, 1, 1, 1\",  # stg_rescale_values\n",
        "            \"[25], [35], [35], [42], [42], [42]\"  # stg_layers_indices\n",
        "        )[0]\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = vae_loader.load_vae(\"ltxv-13b-0.9.7-vae-BF16.safetensors\")[0]\n",
        "\n",
        "        # video_output = img_to_video.generate(\n",
        "        #     positive=positive,\n",
        "        #     negative=negative,\n",
        "        #     vae=vae,\n",
        "        #     image=processed_image,\n",
        "        #     width=width,\n",
        "        #     height=height,\n",
        "        #     length=length,\n",
        "        #     batch_size=1\n",
        "        # )\n",
        "\n",
        "        # sigmas = scheduler.get_sigmas(steps, cfg_scale, 0.95, True, 0.1)[0]\n",
        "        selected_sampler = sampler_select.get_sampler(sampler_name)[0]\n",
        "        # conditioned = conditioning.append(video_output[0], video_output[1], 25.0)\n",
        "\n",
        "        sigmas = float_to_sigmas(\n",
        "            string_to_float(\"1.0000, 0.9937, 0.9875, 0.9812, 0.9750, 0.9094, 0.7250, 0.4219, 0.0\")[0]\n",
        "        )[0]\n",
        "\n",
        "        noise = random_noise.get_noise(seed)[0]\n",
        "\n",
        "        try:\n",
        "\n",
        "            print(\"Generating video...\")\n",
        "\n",
        "            sampled=ltxv_base_sampler.sample(\n",
        "                model,\n",
        "                vae,\n",
        "                width,\n",
        "                height,\n",
        "                length,\n",
        "                guider,\n",
        "                selected_sampler,\n",
        "                sigmas,\n",
        "                noise,\n",
        "                optional_cond_images=loaded_image,\n",
        "                optional_cond_indices=\"0\",\n",
        "                strength=0.8,\n",
        "                crop=\"disabled\",\n",
        "                crf=30,\n",
        "                blur=1\n",
        "            )[0]\n",
        "\n",
        "            # sampled = sampler.sample(\n",
        "            #     model=model,\n",
        "            #     add_noise=True,\n",
        "            #     noise_seed=seed if seed != 0 else random.randint(0, 2**32),\n",
        "            #     cfg=cfg_scale,\n",
        "            #     positive=conditioned[0],\n",
        "            #     negative=conditioned[1],\n",
        "            #     sampler=selected_sampler,\n",
        "            #     sigmas=sigmas,\n",
        "            #     latent_image=video_output[2]\n",
        "            # )[0]\n",
        "\n",
        "            # model_management.soft_empty_cache()\n",
        "            del model\n",
        "            del guider\n",
        "            del noise\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "            try:\n",
        "                print(\"Decodimg Latents...\")\n",
        "                decoded = vae_decode.decode(vae, sampled)[0].detach()\n",
        "                # print(f\"Decoded frames shape: {decoded.shape}\")\n",
        "                # print(\"Latents Decoded!\")\n",
        "                if upscale_video is False:\n",
        "                    del vae\n",
        "                    del sampled\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during decoding: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "            # Reshape to video frames (batch, frames, H, W, C)\n",
        "            # decoded_frames = decoded.reshape(1, length, height, width, 3)\n",
        "\n",
        "            # save_node.save_images(decoded, filename_prefix=\"video_frame\")\n",
        "            decoded = image_scaler.upscale(\n",
        "                decoded,\n",
        "                \"lanczos\",\n",
        "                width_int,\n",
        "                height_int,\n",
        "                \"disabled\"\n",
        "            )[0]\n",
        "\n",
        "            output_path = \"/content/output.mp4\"\n",
        "            frames_np = (decoded.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "            del decoded\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "                for frame in frames_np:\n",
        "                    writer.append_data(frame)\n",
        "\n",
        "            print(f\"\\nBase Video generation complete! Displaying Video...\")\n",
        "            # print(f\"Saved {len(decoded)} frames to ComfyUI output directory\")\n",
        "            # print(f\"Video saved to: {output_path}\")\n",
        "            display_video(output_path)\n",
        "\n",
        "            if upscale_video:\n",
        "                model = unet_loader.load_unet(\"ltxv-13b-0.9.7-distilled-Q6_K.gguf\")[0]\n",
        "                upscale_model = upscale_model_loader.load_model(\n",
        "                    \"ltxv-spatial-upscaler-0.9.7.safetensors\", True, False\n",
        "                )[0]\n",
        "\n",
        "                tiled_guider = stg_guider_advanced.get_guider(\n",
        "                    model,\n",
        "                    conditionedPositive,\n",
        "                    conditionedNegative,\n",
        "                    0.997,  # skip_steps_sigma_threshold\n",
        "                    True,    # cfg_star_rescale\n",
        "                    \"1\",     # sigmas\n",
        "                    \"1\",     # cfg_values\n",
        "                    \"0\",     # stg_scale_values\n",
        "                    \"1\",     # stg_rescale_values\n",
        "                    \"[42]\"   # stg_layers_indices\n",
        "                )[0]\n",
        "\n",
        "                tiled_sigmas = float_to_sigmas(\n",
        "                    string_to_float(\"0.85, 0.7250, 0.6, 0.4219, 0.0\")[0]\n",
        "                )[0]\n",
        "\n",
        "                upscaled_latents = latent_upsampler.upsample_latent(\n",
        "                    sampled, upscale_model, vae\n",
        "                )[0]\n",
        "\n",
        "                adjusted_latents = adain_latent.batch_normalize(\n",
        "                    upscaled_latents, sampled, 0.25\n",
        "                )[0]\n",
        "\n",
        "                del sampled\n",
        "                del upscale_model\n",
        "                del upscaled_latents\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                tiled_noise = random_noise.get_noise(seed)[0]\n",
        "\n",
        "                loaded_image = image_scaler.upscale(\n",
        "                    loaded_image,\n",
        "                    \"lanczos\",\n",
        "                    width,\n",
        "                    height,\n",
        "                    \"disabled\"\n",
        "                )[0]\n",
        "\n",
        "                print(\"Generating high-res video...\")\n",
        "\n",
        "                tiled_output, _ = tiled_sampler.sample(\n",
        "                    model=model,\n",
        "                    vae=vae,\n",
        "                    noise=tiled_noise,\n",
        "                    sampler=selected_sampler,\n",
        "                    sigmas=tiled_sigmas,\n",
        "                    guider=tiled_guider,\n",
        "                    latents=adjusted_latents,\n",
        "                    optional_cond_images=loaded_image,\n",
        "                    horizontal_tiles=1,\n",
        "                    vertical_tiles=1,\n",
        "                    overlap=1,\n",
        "                    latents_cond_strength=0.15,\n",
        "                    boost_latent_similarity=False,\n",
        "                    crop=\"disabled\",\n",
        "                    optional_cond_indices=\"0\",\n",
        "                    images_cond_strengths=\"0.9\"\n",
        "                )\n",
        "\n",
        "                del model\n",
        "                del tiled_guider\n",
        "                del tiled_noise\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                upscaled_latents = tiled_output[\"samples\"]\n",
        "\n",
        "                latent_input = {\n",
        "                    \"samples\": upscaled_latents  # Should be shape [1,4,num_frames,H,W]\n",
        "                }\n",
        "\n",
        "                print(\"Decoding tiles...\")\n",
        "\n",
        "                decoded_frames = vae_decode_tiled.decode(\n",
        "                    vae, latent_input, width, 64, 64, 8\n",
        "                )[0]\n",
        "\n",
        "                decoded_frames = image_scaler.upscale(\n",
        "                    decoded_frames,\n",
        "                    \"lanczos\",\n",
        "                    width_int*2,\n",
        "                    height_int*2,\n",
        "                    \"disabled\"\n",
        "                )[0]\n",
        "\n",
        "                # decoded_frames = film_grain.add_film_grain(\n",
        "                #     decoded_frames, 0.01, 0.5\n",
        "                # )[0]\n",
        "\n",
        "                output_pathU = \"/content/upscaled.mp4\"\n",
        "                frames_npu = (decoded_frames.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "                del vae\n",
        "                del decoded_frames\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                with imageio.get_writer(output_pathU, fps=fps) as writer:\n",
        "                    for frame in frames_npu:\n",
        "                        writer.append_data(frame)\n",
        "\n",
        "                print(f\"\\nHigh-res Video generation complete! Displaying Video...\")\n",
        "                # print(f\"Saved {len(decoded)} frames to ComfyUI output directory\")\n",
        "                # print(f\"Video saved to: {output_path}\")\n",
        "                display_video(output_pathU)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during video generation: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_gpu_memory()\n",
        "\n",
        "\n",
        "def display_video(video_path):\n",
        "    \"\"\"Display video in Colab notebook with proper HTML5 player\"\"\"\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    mp4 = open(video_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "print(\"âœ… Environment Setup Complete!\")"
      ],
      "metadata": {
        "id": "rrXFIT4fMfyJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Image\n",
        "\n",
        "file_uploaded = upload_image()\n",
        "display_upload = False # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded))\n",
        "    else:\n",
        "        print(\"The image format cannot be displayed.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cQFyBT2ZuIuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run Image to Video\n",
        "positive_prompt = \"The lady sits up and waves to the camera.\" # @param {\"type\":\"string\"}\n",
        "negative_prompt = \"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\" # @param {\"type\":\"string\"}\n",
        "# width = 512 # @param {\"type\":\"number\"}\n",
        "# height = 768 # @param {\"type\":\"number\"}\n",
        "width = 0\n",
        "height = 0\n",
        "seed = 0 # @param {\"type\":\"integer\"}\n",
        "steps = 20\n",
        "# steps = 20 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n",
        "cfg_scale = 2.5\n",
        "sampler_name=\"euler_ancestral\" # @param [\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"]\n",
        "frames = 121 # @param {\"type\":\"integer\", \"min\":1, \"max\":120}\n",
        "upscale_video = False # @param {type:\"boolean\"}\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "\n",
        "# @title Run Video Generation\n",
        "generate_video(\n",
        "        image_path=file_uploaded,\n",
        "        positive_prompt=positive_prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        seed=seed,\n",
        "        steps=steps,\n",
        "        cfg_scale=cfg_scale,\n",
        "        sampler_name=sampler_name,\n",
        "        length=frames,\n",
        "        upscale_video=upscale_video\n",
        ")\n",
        "clear_gpu_memory()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "roC59_oNNflb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}